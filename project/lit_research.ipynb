{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Book: cat\n",
      "Similar to MITCH HEDBERG: COMEDY CENTRAL SPECIAL (1999) 4563 total words innovation rate: 0.24698663160201623\n",
      "First 10 words: ['For', 'the', 'most', 'wild', 'yet', 'most', 'homely', 'narrative', 'which', 'I', 'am', 'about', 'to', 'pen', 'I', 'neither', 'expect', 'nor', 'solicit', 'belief', 'Mad', 'indeed', 'would', 'I', 'be', 'to', 'expect', 'it', 'in', 'a', 'case', 'where', 'my', 'very', 'senses', 'reject', 'their', 'own', 'evidence', 'Yet', 'mad', 'am', 'I', 'not', 'and', 'very', 'surely', 'do', 'I', 'not']\n",
      "Last 10 words: ['hangman', 'I', 'had', 'walled', 'the', 'monster', 'up', 'within', 'the', 'tomb']\n",
      "The total number of words is:  4036  and the number of unique words is:  1306\n",
      "Unique Words that appear once: 877\n",
      "Unique Words that appear twice: 201\n",
      "Unique Words that appear three times: 79\n",
      "\n",
      "-------\n",
      "\n",
      "Thus our n_1^g estimate is:  0.672\n",
      "Thus our n_2^g estimate is:  0.154\n",
      "Thus our n_3^g estimate is:  0.06\n",
      "\n",
      "-------------\n",
      "\n",
      "Estimated Innovation Rate (rho): 0.3236\n",
      "Theoretical n_1: 0.597, n_2: 0.171, n_3: 0.077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.323587710604559),\n",
       " (np.float64(0.59651197162282),\n",
       "  np.float64(0.17149091012324122),\n",
       "  np.float64(0.07658599454219671)))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "def process_ulysses(file_path):\n",
    "    try:\n",
    "        mat_contents = scipy.io.loadmat(file_path)\n",
    "        words = mat_contents['words'].squeeze()  # Convert to 1D array\n",
    "        counts = mat_contents['counts'].squeeze()  # Convert to 1D array\n",
    "        if len(words) == len(counts):\n",
    "            # Create a DataFrame from the words and counts\n",
    "            df = pd.DataFrame({\n",
    "                'Word': words,\n",
    "                'Count': counts\n",
    "            })\n",
    "            return df\n",
    "        else:\n",
    "            print(\"The lengths of 'words' and 'counts' do not match.\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def read_word_counts(book_title):\n",
    "    if book_title == 'compte':  # Le Compte de Monte Cristo (in French)\n",
    "        url, word_start, word_end = \"https://www.gutenberg.org/cache/epub/17989/pg17989.txt\", 313, 131872  # Adjust word indices\n",
    "    if book_title == 'fall': \n",
    "        url, word_start, word_end = \"https://www.gutenberg.org/cache/epub/2148/pg2148-images.html#chap2.8\", 36604, 44001  # Adjust word indices\n",
    "    if book_title == 'tell':\n",
    "        url, word_start, word_end = \"https://www.gutenberg.org/cache/epub/2148/pg2148-images.html#chap2.20\", 90883, 93077\n",
    "    if book_title == 'pride':\n",
    "        url, word_start, word_end = \"https://www.gutenberg.org/cache/epub/42671/pg42671.txt\", 337, 123673  # Adjust word indices\n",
    "    if book_title == 'bartleby':\n",
    "        url, word_start, word_end = \"https://www.gutenberg.org/cache/epub/11231/pg11231-images.html\", 830, 16032\n",
    "    if book_title == 'cat':\n",
    "        # Edgar Allan Poe's The Black Cat\n",
    "        # Similar to MITCH HEDBERG: COMEDY CENTRAL SPECIAL (1999) 4563 total words \n",
    "        url, word_start, word_end = \"https://www.gutenberg.org/cache/epub/2148/pg2148-images.html#chap2.7\", 32520, 36556\n",
    "        print('Similar to MITCH HEDBERG: COMEDY CENTRAL SPECIAL (1999) 4563 total words innovation rate: 0.24698663160201623')\n",
    "    # Fetch the content\n",
    "    response = urllib.request.urlopen(url)\n",
    "    content = response.read().decode('utf-8')  # Decode the bytes to string\n",
    "    # Use nltk's RegexpTokenizer to tokenize words from the entire content\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(content)  # Tokenize the entire content\n",
    "    # Select the word range based on word indices (not characters)\n",
    "    words = words[word_start:word_end]\n",
    "    # Display the first and last 10 words in the selected range\n",
    "    # print(words.index('BLACK'))\n",
    "    # print(\"First 10 words:\", words[:50])\n",
    "    # print(\"Last 10 words:\", words[-10:])\n",
    "    # Count occurrences of each word\n",
    "    word_counts = pd.Series(words).value_counts().reset_index()\n",
    "    word_counts.columns = ['Word', 'Count']\n",
    "    # Sort word_counts by count in ascending order\n",
    "    word_counts = word_counts.sort_values(by='Count', ascending=True).reset_index(drop=True)\n",
    "    return word_counts\n",
    "# Function to estimate innovation rate\n",
    "def estimate_innovation_rate(counts_df):\n",
    "    counts_df = counts_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    counts_df['Rank'] = np.arange(1, len(counts_df) + 1)\n",
    "    rho_est = len(counts_df)/counts_df[\"Count\"].sum()\n",
    "    return rho_est, counts_df\n",
    "# Function to calculate theoretical and empirical values\n",
    "def calculate_theoretical_values(rho_est):\n",
    "    n_1 = 1 / (2 - rho_est)\n",
    "    n_2 = n_1 * (1 - rho_est) / (1 + (1 - rho_est) * 2)\n",
    "    n_3 = n_2 * (2 - 2*rho_est) / (1 + (1 - rho_est) * 3)   \n",
    "    return n_1, n_2, n_3\n",
    "def get_word_counts(df):\n",
    "    count_once = 0\n",
    "    count_twice = 0\n",
    "    count_thrice = 0\n",
    "    # Loop through the dataframe and increment the respective counter\n",
    "    for count in df['Count']:\n",
    "        if count == 1:\n",
    "            count_once += 1\n",
    "        elif count == 2:\n",
    "            count_twice += 1\n",
    "        elif count == 3:\n",
    "            count_thrice += 1\n",
    "    # Print the results\n",
    "    print(\"The total number of words is: \", df[\"Count\"].sum(), \" and the number of unique words is: \", len(df))\n",
    "    print(f\"Unique Words that appear once: {count_once}\")\n",
    "    print(f\"Unique Words that appear twice: {count_twice}\")\n",
    "    print(f\"Unique Words that appear three times: {count_thrice}\")\n",
    "    print(\"\\n-------\\n\")\n",
    "    print(\"Thus our n_1^g estimate is: \", round(count_once / len(df), 3))\n",
    "    print(\"Thus our n_2^g estimate is: \", round(count_twice / len(df), 3))\n",
    "    print(\"Thus our n_3^g estimate is: \", round(count_thrice / len(df), 3))\n",
    "    print(\"\\n-------------\\n\")\n",
    "    # Innovation rate estimate\n",
    "    rho_est = len(df)/df[\"Count\"].sum()\n",
    "    return rho_est\n",
    "# Main execution function for each book\n",
    "def analyze_book(book_title):\n",
    "    print(f\"\\nBook: {book_title}\")\n",
    "    # if book title  is ulysses call process_ulysses\n",
    "    if book_title == 'ulysses':\n",
    "        file_path = '/Users/robinwoodfamily/Downloads/ulysses.mat'\n",
    "        mat_data = process_ulysses(file_path)\n",
    "        innovation_rate, processed_counts_df = estimate_innovation_rate(mat_data)\n",
    "        theoretical_values_est = calculate_theoretical_values(0.1150)\n",
    "        get_word_counts(processed_counts_df)\n",
    "        print(f\"Estimated Innovation Rate (rho): {innovation_rate:.4f}\")\n",
    "        print(f\"Theoretical n_1: {theoretical_values_est[0]:.3f}, n_2: {theoretical_values_est[1]:.3f}, n_3: {theoretical_values_est[2]:.3f}\")\n",
    "        return \n",
    "    word_counts_df = read_word_counts(book_title)\n",
    "    get_word_counts(word_counts_df)\n",
    "    innovation_rate, processed_counts_df = estimate_innovation_rate(word_counts_df)\n",
    "    # Calculate theoretical and empirical values\n",
    "    theoretical_values_est = calculate_theoretical_values(innovation_rate)\n",
    "    print(f\"Estimated Innovation Rate (rho): {innovation_rate:.4f}\")\n",
    "    print(f\"Theoretical n_1: {theoretical_values_est[0]:.3f}, n_2: {theoretical_values_est[1]:.3f}, n_3: {theoretical_values_est[2]:.3f}\")\n",
    "    return innovation_rate, theoretical_values_est\n",
    "\n",
    "# Analyze all three books\n",
    "# analyze_book('compte')\n",
    "# analyze_book('pride')\n",
    "analyze_book('cat')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyze_book('cat')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
