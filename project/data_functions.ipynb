{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/robinwoodfamily/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/robinwoodfamily/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word  counts     probs  total_unique\n",
      "0        I     241  0.055237          1117\n",
      "1        a     171  0.039193          1117\n",
      "2      the     123  0.028192          1117\n",
      "3       to     122  0.027962          1117\n",
      "4      you      79  0.018107          1117\n",
      "..     ...     ...       ...           ...\n",
      "95   never       8  0.001834          1117\n",
      "96     now       7  0.001604          1117\n",
      "97  comedy       7  0.001604          1117\n",
      "98  around       7  0.001604          1117\n",
      "99    need       7  0.001604          1117\n",
      "\n",
      "[100 rows x 4 columns]\n",
      "Unique words: 1117\n",
      "Total words: 4363\n",
      "0.25601650240660095\n",
      "    word  counts     probs  total_unique\n",
      "0      I     443  0.061019          1485\n",
      "1    you     208  0.028650          1485\n",
      "2      a     205  0.028237          1485\n",
      "3     it     187  0.025758          1485\n",
      "4    the     164  0.022590          1485\n",
      "5   know     131  0.018044          1485\n",
      "6     to     121  0.016667          1485\n",
      "7   that     117  0.016116          1485\n",
      "8    And      99  0.013636          1485\n",
      "9   like      96  0.013223          1485\n",
      "10   and      96  0.013223          1485\n",
      "11    of      85  0.011708          1485\n",
      "12    is      81  0.011157          1485\n",
      "13    in      80  0.011019          1485\n",
      "14   You      72  0.009917          1485\n",
      "15   was      70  0.009642          1485\n",
      "16    my      67  0.009229          1485\n",
      "17    It      64  0.008815          1485\n",
      "18   for      63  0.008678          1485\n",
      "19    me      54  0.007438          1485\n",
      "20  just      53  0.007300          1485\n",
      "21  they      49  0.006749          1485\n",
      "22    so      46  0.006336          1485\n",
      "23    be      46  0.006336          1485\n",
      "24    he      45  0.006198          1485\n",
      "25    re      45  0.006198          1485\n",
      "26    do      44  0.006061          1485\n",
      "27  what      42  0.005785          1485\n",
      "28   don      41  0.005647          1485\n",
      "29    on      39  0.005372          1485\n",
      "Unique words: 1485\n",
      "Total words: 7260\n",
      "0.20454545454545456\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer,TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/marobinette/pocs/main/project/comedy_data_10_31.csv'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "df = df[['title', 'length', 'transcript']]\n",
    "\n",
    "def trimTitle(title):\n",
    "    trimmedTitle = title.split('â€“')[0]\n",
    "    # remove whitespace at the end\n",
    "    trimmedTitle = trimmedTitle.rstrip()\n",
    "    return trimmedTitle\n",
    "\n",
    "# apply trimTitle to the title column\n",
    "df['title'] = df['title'].apply(trimTitle)  \n",
    "\n",
    "def export_comedian_word_data(word_data, comedian):\n",
    "    \"\"\"\n",
    "    Exports the word data for a specific comedian to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - word_data (DataFrame): DataFrame containing words, counts, total unique words, and probabilities.\n",
    "    - comedian (str): Name of the comedian to label the CSV file.\n",
    "    \"\"\"\n",
    "    word_data = word_data.rename(columns={'word': 'types'})\n",
    "    word_data = word_data[['types', 'counts', 'total_unique', 'probs']]    \n",
    "    filename = f\"{comedian}.csv\"\n",
    "    word_data.to_csv(filename, index=False)\n",
    "    print(f\"Data exported to {filename} successfully.\")\n",
    "\n",
    "def get_word_data(words):\n",
    "    \"\"\"\n",
    "    Generates word frequency data including counts and probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "    - words (list): List of words to analyze for frequency and probability.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: DataFrame with word counts, probabilities, and total unique word count.\n",
    "    \"\"\"\n",
    "    word_data = pd.Series(words).value_counts().reset_index()\n",
    "    word_data.columns = ['word', 'counts']        \n",
    "    word_data['probs'] = word_data['counts'] / word_data['counts'].sum()    \n",
    "    word_data['total_unique'] = len(word_data)\n",
    "    word_data = word_data.sort_values(by='counts', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return word_data\n",
    "\n",
    "def tokenize_words(text, remove_stop_words=False):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into words, with an option to remove stopwords.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str or list): Text or list of strings to tokenize.\n",
    "    - remove_stop_words (bool): Whether to remove stopwords from the tokens.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of tokenized words.\n",
    "    \"\"\"\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # Tokenize text while preserving contractions.\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove all words in brackets or punctuation.\n",
    "    tokenizer = TweetTokenizer()\n",
    "    words = tokenizer.tokenize(text)\n",
    "    words = [re.sub(r\"[^\\w\\s]\", '', word) for word in words]\n",
    "\n",
    "    # Filter out empty strings resulting from punctuation removal.\n",
    "    words = [word for word in words if word]\n",
    "\n",
    "    # Remove one-letter words except \"I\", \"a\", or \"A\"\n",
    "    words = [word for word in words if len(word) > 1 or word in ['I', 'a', 'A']]\n",
    "\n",
    "    # Remove stopwords if the option is selected.\n",
    "    if remove_stop_words:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def calculate_innovation_rate(words):\n",
    "    \"\"\"\n",
    "    Calculates the innovation rate as the ratio of unique words to the total words.\n",
    "    \n",
    "    Parameters:\n",
    "    - words (list): List of tokenized words.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Innovation rate.\n",
    "    \"\"\"\n",
    "    unique_words = set(words)\n",
    "    print(f\"Unique words: {len(unique_words)}\")\n",
    "    total_words = len(words)\n",
    "    print(f\"Total words: {total_words}\")\n",
    "    innovation_rate = len(unique_words) / total_words if total_words > 0 else 0\n",
    "    return innovation_rate\n",
    "\n",
    "def get_innovation_rate_data(df):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with title, innovation rate, and length of set for each comedian.\n",
    "    \"\"\"\n",
    "    titles = []\n",
    "    lengths = []  # Renamed to avoid overwriting\n",
    "    innovation_rates = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        title = row['title']\n",
    "        length = row['length']  # Length of the set\n",
    "        transcript = row['transcript']\n",
    "        \n",
    "        words = tokenize_words(transcript, remove_stop_words=False)\n",
    "        innovation_rate = calculate_innovation_rate(words) * 100  # Convert to percentage\n",
    "        \n",
    "        # Append data to lists\n",
    "        titles.append(title)\n",
    "        lengths.append(length)\n",
    "        innovation_rates.append(innovation_rate)\n",
    "    \n",
    "    # Create a new DataFrame with title, length, and innovation_rate columns\n",
    "    innovation_rate_df = pd.DataFrame({\n",
    "        'title': titles,\n",
    "        'length': lengths,  # Corrected list name here\n",
    "        'innovation_rate': innovation_rates\n",
    "    })\n",
    "\n",
    "    return innovation_rate_df\n",
    "\n",
    "\n",
    "def plot_zipf_distribution(df, title):\n",
    "    \"\"\"\n",
    "    Plots the Zipf distribution for the given comedian's transcript.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): DataFrame containing comedy data with titles and transcripts.\n",
    "    - title (str): The comedian's name for which to plot the distribution.\n",
    "    \"\"\"\n",
    "    # Filter the dataframe for the given comedian\n",
    "    transcript = df[df['title'] == title]['transcript'].values[0]\n",
    "    \n",
    "    # Tokenize the words\n",
    "    words = tokenize_words(transcript, remove_stop_words=False)\n",
    "    \n",
    "    # Generate word frequency data\n",
    "    word_data = get_word_data(words)\n",
    "    \n",
    "    # Rank the words by frequency\n",
    "    word_data['rank'] = np.arange(1, len(word_data) + 1)\n",
    "    \n",
    "    # Log-transform rank and frequency\n",
    "    log_rank = np.log10(word_data['rank'])\n",
    "    log_freq = np.log10(word_data['counts'])\n",
    "    \n",
    "    # Perform linear regression to calculate the slope\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(log_rank, log_freq)\n",
    "    \n",
    "    # Plot the Zipf distribution (log-log scale)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(word_data['rank'], word_data['counts'], marker=\".\", linestyle='none', color=\"grey\", label=f'(slope={slope:.2f})')\n",
    "    \n",
    "    plt.xlabel('Rank (log scale)')\n",
    "    plt.ylabel('Frequency (log scale)')\n",
    "    plt.title(f'Zipf Law - {title}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_innovation_rate(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(innovation_rate_df['length'], innovation_rate_df['innovation_rate'], alpha=0.7)\n",
    "    plt.xlabel('Length of Set')\n",
    "    plt.ylabel('Innovation Rate (%)')\n",
    "    plt.title('Innovation Rate vs. Length of Set')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_innovation_rate_data_with_word_count(df):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with title, innovation rate, and word count of each comedian's set.\n",
    "    \"\"\"\n",
    "    titles = []\n",
    "    word_counts = []\n",
    "    innovation_rates = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        title = row['title']\n",
    "        transcript = row['transcript']\n",
    "        \n",
    "        words = tokenize_words(transcript, remove_stop_words=False)\n",
    "        word_count = len(words)\n",
    "        innovation_rate = calculate_innovation_rate(words) * 100  # Convert to percentage\n",
    "        \n",
    "        # Append data to lists\n",
    "        titles.append(title)\n",
    "        word_counts.append(word_count)\n",
    "        innovation_rates.append(innovation_rate)\n",
    "    \n",
    "    # Create a new DataFrame with title, word_count, and innovation_rate columns\n",
    "    innovation_rate_df = pd.DataFrame({\n",
    "        'title': titles,\n",
    "        'word_count': word_counts,\n",
    "        'innovation_rate': innovation_rates\n",
    "    })\n",
    "\n",
    "    return innovation_rate_df\n",
    "\n",
    "def plot_innovation_rate_by_word_count(df):\n",
    "    \"\"\"\n",
    "    Plots innovation rate against the number of words in each transcript.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): DataFrame containing title, word_count, and innovation_rate.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['word_count'], df['innovation_rate'], alpha=0.7)\n",
    "    plt.xlabel('Transcript Length (Word Count)')\n",
    "    plt.ylabel('Innovation Rate (%)')\n",
    "    plt.title('Innovation Rate vs. Transcript Length')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_vocabulary_growth(words):\n",
    "    \"\"\"\n",
    "    Calculates vocabulary size as a function of word count for a given transcript.\n",
    "    Returns a list of tuples where each tuple contains the count of words and unique words.\n",
    "    \"\"\"\n",
    "    vocab_growth = []\n",
    "    unique_words = set()\n",
    "    for i, word in enumerate(words, 1):\n",
    "        unique_words.add(word)\n",
    "        vocab_growth.append((i, len(unique_words)))\n",
    "    return np.array(vocab_growth)\n",
    "\n",
    "def plot_heaps_law(df, title):\n",
    "    \"\"\"Plots Heaps' Law based on vocabulary growth for a single comedy set.\"\"\"\n",
    "    transcript = df[df['title'] == title]['transcript'].values[0]\n",
    "    words = tokenize_words(transcript, remove_stop_words=False)\n",
    "    vocab_growth = calculate_vocabulary_growth(words)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(vocab_growth[:, 0], vocab_growth[:, 1], color='grey')\n",
    "    plt.xlabel(\"Total Words (N)\")\n",
    "    plt.ylabel(\"Unique Words\")\n",
    "    plt.title(f\"Heaps' Law - '{title}'\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate the updated DataFrame and plot\n",
    "\n",
    "\n",
    "def plot_zipf_and_heaps_all_comedians(df):\n",
    "    \"\"\"\n",
    "    Plots Zipf's Law and Heaps' Law side by side for each comedian in the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): DataFrame containing comedy data with titles, lengths, and transcripts.\n",
    "    \"\"\"\n",
    "    num_comedians = len(df)\n",
    "    fig, axes = plt.subplots(num_comedians, 2, figsize=(12, 6 * num_comedians))\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        title = row['title']\n",
    "        transcript = row['transcript']\n",
    "        \n",
    "        # Tokenize words\n",
    "        words = tokenize_words(transcript, remove_stop_words=False)\n",
    "        \n",
    "        # Prepare Zipf's Law data\n",
    "        word_data = get_word_data(words)\n",
    "        word_data['rank'] = np.arange(1, len(word_data) + 1)\n",
    "        log_rank = np.log10(word_data['rank'])\n",
    "        log_freq = np.log10(word_data['counts'])\n",
    "        \n",
    "        # Perform linear regression for Zipf's Law slope\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(log_rank, log_freq)\n",
    "        \n",
    "        # Plot Zipf's Law (log-log scale)\n",
    "        axes[idx, 0].loglog(word_data['rank'], word_data['counts'], marker=\".\", linestyle='none', color=\"grey\")\n",
    "        axes[idx, 0].set_title(title)\n",
    "        axes[idx, 0].set_xlabel('Rank (log scale)')\n",
    "        axes[idx, 0].set_ylabel('Frequency (log scale)')\n",
    "        \n",
    "        # Prepare Heaps' Law data\n",
    "        vocab_growth = calculate_vocabulary_growth(words)\n",
    "        \n",
    "        # Plot Heaps' Law\n",
    "        axes[idx, 1].plot(vocab_growth[:, 0], vocab_growth[:, 1], color='grey')\n",
    "        axes[idx, 1].set_title(title)\n",
    "        axes[idx, 1].set_xlabel(\"Total Words (N)\")\n",
    "        axes[idx, 1].set_ylabel(\"Unique Words (V)\")\n",
    "    \n",
    "    # Adjust layout for readability\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# innovation_rate_df = get_innovation_rate_data(df)\n",
    "# plot_innovation_rate(innovation_rate_df)\n",
    "\n",
    "# innovation_rate_df = get_innovation_rate_data_with_word_count(df)\n",
    "# plot_innovation_rate_by_word_count(innovation_rate_df)\n",
    "\n",
    "# plot_zipf_and_heaps_all_comedians(df)\n",
    "# print(df['title'].unique())\n",
    "\n",
    "# compare with The black Cat by Edgar Allan Poe\n",
    "mitch_words = tokenize_words(df[df['title'] == 'MITCH HEDBERG: COMEDY CENTRAL SPECIAL (1999)']['transcript'].values[0], remove_stop_words=False)\n",
    "mitch_word_data = get_word_data(mitch_words)\n",
    "print(mitch_word_data.head(100))\n",
    "print(calculate_innovation_rate(mitch_words))\n",
    "\n",
    "jesus_is_magic = tokenize_words(df[df['title'] == 'SARAH SILVERMAN: JESUS IS MAGIC (2005)']['transcript'].values[0], remove_stop_words=False)\n",
    "jesus_is_magic_word_data = get_word_data(jesus_is_magic)\n",
    "print(jesus_is_magic_word_data.head(30))\n",
    "print(calculate_innovation_rate(jesus_is_magic))\n",
    "\n",
    "# print(df[df['length'] > 70]['title'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
